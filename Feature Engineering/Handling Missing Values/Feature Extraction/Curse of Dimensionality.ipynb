{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7eb028",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality\n",
    "The Curse of Dimensionality is termed by mathematician R. ... The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b60a30",
   "metadata": {},
   "source": [
    "## What happens in the large amount of dimensions (features) ?\n",
    "\n",
    "### 1. Performance decreases\n",
    "* Because when the dimension increases the data points go away from each other so the distance in between the points increases due to this when i apply the distance culculating based algorithm upon this data so the performance decreases because there is a huge distance between data points\n",
    "\n",
    "### 2. computation time expands OR Increases\n",
    "* This is simple to understand. Just imagine that you have huge amount of features. When you measure the distance of every point to another point and let's say there are 700 data points so you according to the distance equation **V=S/t** so when we calculate the speed of coputation or running the code so it's directly proportional to distance so it takes hug time to compute and require more speed for this computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae971581",
   "metadata": {},
   "source": [
    "# Methods for solving this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a83231",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset. More input features often make a predictive modeling task more challenging to model, more generally referred to as the curse of dimensionality.\n",
    "\n",
    "- Now there are two types of Dimensionality Reduction.\n",
    "\n",
    "* Feature Selection\n",
    "* Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c20321",
   "metadata": {},
   "source": [
    "## 1-Feature Selection\n",
    "* Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "Overview.\n",
    "* There are three types of feature selection: **Wrapper methods** (forward, backward, and stepwise selection), **Filter methodsD** (ANOVA, Pearson correlation, variance thresholding), and **Embedded methods** (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c76505",
   "metadata": {},
   "source": [
    "In feature selection we select only those features for our model which gives the best accuracy for prediction. So here i wrote some techniques to do Feature Selection.\n",
    "* **Forward Selection**\n",
    "* **Backward Elimination**\n",
    "* **Bidirectional Elimination**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f8d31",
   "metadata": {},
   "source": [
    "## 2-Feature Extraction\n",
    "Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. A characteristic of these large data sets is a large number of variables that require a lot of computing resources to process.\n",
    "In Feature extraction we create a new linear feature by using given features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e775cf6",
   "metadata": {},
   "source": [
    "In Feature Extraction there are three techniques.\n",
    "* **PCA (Principle Component Analysis)**\n",
    "* **LDA (Linear Discriminant Analysis)**\n",
    "* **t-SNE (t-Distributed Stochastic Neighbor Embedding)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3d873",
   "metadata": {},
   "source": [
    "# PCA \n",
    "* Principal Component Analysis, or PCA,It's a feature extraction texhnique and it is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "* As there is only input in data no output so this is an un supervised machine learning problem.\n",
    "* It is too old and reliable technique.And it's underline maths is too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be484275",
   "metadata": {},
   "source": [
    "Principal Component Analysis is an unsupervised learning algorithm that is used for the dimensionality reduction in machine learning. ... PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7bcf4",
   "metadata": {},
   "source": [
    "### Benifits\n",
    "* Faster execution of algorithms\n",
    "* Visualization becomes easy on less dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad83bf4",
   "metadata": {},
   "source": [
    "<img title=\"PCA Formula for finding the components which has maximum variance\" src=\"images/pca.png\" height=500, width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9447185",
   "metadata": {},
   "source": [
    "* By using this (Variance of projection on a unit vector) we find out those unit vectors which give us the maximum variance.\n",
    "* So in simple terms i can say that PCA provide us those directions of vector or those vectors which can give us the maximum variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533bae2",
   "metadata": {},
   "source": [
    "<img title=\"PCA Projection\" src=\"images/pca projection.jpeg\" height=500 width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b61c4c",
   "metadata": {},
   "source": [
    "#### So this becomes an Optimization problem\n",
    "In mathematics, computer science and economics, an optimization problem is the problem of finding the best solution from all feasible solutions. ... A problem with continuous variables is known as a continuous optimization, in which an optimal value from a continuous function must be found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b4d10",
   "metadata": {},
   "source": [
    "***SO next i will be discussed about what the solution people get after using PCA but first we need to understand about***\n",
    "* What is covariance & Covariance Matrix\n",
    "* What is Eigen Vectors & Eigen Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c50b08",
   "metadata": {},
   "source": [
    "# When PCA Doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aebde46",
   "metadata": {},
   "source": [
    "* When your variance is equal on both sides (x-axis and y-axis) its mean that when you find the eigen vector then it should be having same problem as before\n",
    "* v shape data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a30dd",
   "metadata": {},
   "source": [
    "## Covariance and Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca46584",
   "metadata": {},
   "source": [
    "### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609283f8",
   "metadata": {},
   "source": [
    "Variance and covariance are mathematical terms frequently used in statistics and probability theory. Variance refers to the spread of a data set around its mean value, while a covariance refers to the measure of the directional relationship between two random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ddf1bd",
   "metadata": {},
   "source": [
    "***Variance and Covariance Formula***\n",
    "<img title=\"Variance\" src=\"images/Variance-Analysis-Formula.jpg\" height=400 width=400 />\n",
    "<img title=\"Covariance\" src=\"images/Covariance-Formula.jpg\" height=400 width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269247c",
   "metadata": {},
   "source": [
    "***Correlation and covariance are mostly same thing, there is only a restriction in correlation that the value of carrelation is between -1 to 1 but in covariance there is no restriction like that.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba31cef",
   "metadata": {},
   "source": [
    "### Covariance Matrix\n",
    "In probability theory and statistics, a covariance matrix (also known as auto-covariance matrix, dispersion matrix, variance matrix, or varianceâ€“covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dd4f3",
   "metadata": {},
   "source": [
    "In the Covariance the diagonal values are variance and non-diagonal values are covariance.\n",
    "**cov (a,b) = cov (b,a)**\n",
    "<img title=\"Covariance Matrix\" src=\"images/covariance-matrix.png\" height=400 width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de7213",
   "metadata": {},
   "source": [
    "Covariance is important because it tells us the variance (spread) of every axis. and it also tells us the relationship between any two pair of axis or any two features that is there is -ve relation or +ve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947078b",
   "metadata": {},
   "source": [
    "The values of variance in covariance matrix are act like a mirror. As you can see in the image that the values of above side and below sides are opposite to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db4e159",
   "metadata": {},
   "source": [
    "## Eigen Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d37eab0",
   "metadata": {},
   "source": [
    "***Eigen Vectors are those vectors when we apply the transformation upon it then the direction of those vectors will not change.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e6b41",
   "metadata": {},
   "source": [
    "https://www.geogebra.org/m/YCZa8TAH Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344dbca3",
   "metadata": {},
   "source": [
    "**Point to be noted here that matrixes are the linear transformation which create changes in the quardinate system.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b4e52",
   "metadata": {},
   "source": [
    "## Eigen Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14144e7a",
   "metadata": {},
   "source": [
    "***Eigen values are those values when transformation is apply and the stretched of vector decrease or increase .So the increasing and decreasing value of vector magnitude is called eigen values.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3467378",
   "metadata": {},
   "source": [
    "<img title=\"EigenVector & EigenValues\" src=\"images/eigenvector.jpg\" height=500 width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37f018",
   "metadata": {},
   "source": [
    "**It is show that.when we apply this equation then it is same as we just multiply a eigenvector with a scalar . means that there is no change happens in direction only the magnitued will change.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d367b",
   "metadata": {},
   "source": [
    "**Blog**\n",
    "\n",
    "https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/#:~:text=covariance%20matrix%20captures%20the%20spread%20of%20N%2Ddimensional%20data.&text=Figure%203.,is%20captured%20by%20the%20variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5f674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
