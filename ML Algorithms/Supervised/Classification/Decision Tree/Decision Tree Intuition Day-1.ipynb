{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02fb4010",
   "metadata": {},
   "source": [
    "# <ins>Decision Tree Classification Algorithm</ins>\n",
    "* Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.\n",
    "* In a Decision tree, there are two nodes, which are the Decision Node and Leaf Node. Decision nodes are used to make any decision and have multiple branches, whereas Leaf nodes are the output of those decisions and do not contain any further branches.\n",
    "* The decisions or the test are performed on the basis of features of the given dataset.\n",
    "* It is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions.\n",
    "* It is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.\n",
    "* In order to build a tree, we use the CART algorithm, which stands for Classification and Regression Tree algorithm.\n",
    "* A decision tree simply asks a question, and based on the answer (Yes/No), it further split the tree into subtrees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce046a6",
   "metadata": {},
   "source": [
    "***Below diagram explains the general structure of a decision tree:***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33f2cd",
   "metadata": {},
   "source": [
    "<img src='images/decision-tree-classification-algorithm.png' height=600 width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d401cdf",
   "metadata": {},
   "source": [
    "[Decision Tree Complete Info article](https://www.javatpoint.com/machine-learning-decision-tree-classification-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258a5b7",
   "metadata": {},
   "source": [
    "<img src='images/decision tree example.png' height=800 width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ef764",
   "metadata": {},
   "source": [
    "<img src='images/example part 2.png' height=800 width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897ea22",
   "metadata": {},
   "source": [
    "## For Regression Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f820d0",
   "metadata": {},
   "source": [
    "<img src='images/decision tree for numerical data.png' height=900 width=900 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2b01c",
   "metadata": {},
   "source": [
    "<img src='images/Geometrical intuition.png' height=900 width=900 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0bc9f5",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "* if we are speaking prgrammatically, Decision trees are nothing but a giant structure of nested if-else conditions.\n",
    "* if we speaking Mathematically, Decision trees use hyperplane which run parallel to any one of the axes to cut your coordinate system into hyper cuboids. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49a150",
   "metadata": {},
   "source": [
    "## Advantages\n",
    "* Intuitive and easy to understand.It is simple to understand as it follows the same process which a human follow while making any decision in real-life.\n",
    "* Minimal data preparation is required. There is less requirement of data cleaning compared to other algorithms. \n",
    "* It helps to think about all the possible outcomes for a problem. \n",
    "* It can be very useful for solving decision-related problems.\n",
    "* The cost of using the tree for inference is Logarithmic in the number of data points used to train the tree.So it will be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc9769",
   "metadata": {},
   "source": [
    "## Disadvantages\n",
    "* It may have an overfitting issue, which can be resolved using the Random Forest algorithm. \n",
    "* Prone to errors for imbalanced datasets.\n",
    "* The decision tree contains lots of layers, which makes it complex. \n",
    "* For more class labels, the computational complexity of the decision tree may increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e8b20",
   "metadata": {},
   "source": [
    "### Fun Example\n",
    "[Akinator Game](https://en.akinator.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55330876",
   "metadata": {},
   "source": [
    "# <ins>Entropy</ins>\n",
    "In the most layman terms Entropy is nothing but the measure of disorder. Or you can also call it the measure of purity/impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df7e8c",
   "metadata": {},
   "source": [
    "Entropy as happens when the randomness increases means that when spontaneous terms happen.So greater randomness means more entropy and less randomness means less entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74437f2",
   "metadata": {},
   "source": [
    "***Here we can see that the entropy of Gas is more because the randomness in Gas molecule is greater than others.***\n",
    "<img src='images/Entropy-change-in-thermodynamics.png' height=600 width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250746a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ \\hat{y} = \\hat{\\beta}_0 + \\sum \\limits_{j=1} ^{p} X_{j} \\hat {\\beta}_j $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872dbf64",
   "metadata": {},
   "source": [
    "<img src='images/entropy for dataset.png' height=700 width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b813d00",
   "metadata": {},
   "source": [
    "<img src='images/entropy dataset 2.png' height=600 width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476113f",
   "metadata": {},
   "source": [
    "## Observation\n",
    "* More the uncertainty more is entropy.\n",
    "* For the two class problem the min entropy is 0 and the maximum entropy is 1.\n",
    "* For more than two classes the min entropy is 0 but the maximum can be greater than 1.\n",
    "* Both $ log2 $ or $ loge $ can be used to calculate entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d07567",
   "metadata": {},
   "source": [
    "<img title='Entropy VS Probability' src='images/entropy vs probability.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a90687",
   "metadata": {},
   "source": [
    "## Entropy for Continuous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6304d5d",
   "metadata": {},
   "source": [
    "<img src='images/entropy for continuous variables.png' height=600 width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cefa2",
   "metadata": {},
   "source": [
    "# <ins>Information gain</ins>\n",
    "Information gain is the reduction in entropy or surprise by transforming a dataset and is often used in training decision trees. Information gain is calculated by comparing the entropy of the dataset before and after a transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7eb38",
   "metadata": {},
   "source": [
    "<img title='Information Gain' src='images/information gain formula.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a9328",
   "metadata": {},
   "source": [
    "<img src='images/entropy dataset 3.png' height=600 width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47af3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into 3 parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f95b81",
   "metadata": {},
   "source": [
    "<img src='images/entropy dataset-4.png' height=600 width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446d58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaf node should be consider of those feature which has 0 entropy value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c418a78",
   "metadata": {},
   "source": [
    "<img src='images/entropy dataset 5.png' height=600 width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa8568",
   "metadata": {},
   "source": [
    "<img src='images/calculating information gain.png' height=600 width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e85859",
   "metadata": {},
   "source": [
    "##### Step5 :Calculate information Gain for all the column:\n",
    "***Whichever column has the highest information Gain (maximum decrease in entropy) the algorithm will select that column to split the data***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650f058",
   "metadata": {},
   "source": [
    "<img src='images\\find information gain recursively.png' height=600 width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c7ede",
   "metadata": {},
   "source": [
    "# <ins>Gini Impurity</ins>\n",
    "The Gini impurity measure is one of the methods used in decision tree algorithms to decide the optimal split from a root node, and subsequent splits. ... A Gini Impurity measure will help us make this decision. Def: Gini Impurity tells us what is the probability of misclassifying an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d818d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
