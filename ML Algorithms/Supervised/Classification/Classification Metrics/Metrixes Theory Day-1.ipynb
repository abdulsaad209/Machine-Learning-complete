{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c45481",
   "metadata": {},
   "source": [
    "# Accuracy Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8591b",
   "metadata": {},
   "source": [
    "Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition: **Accuracy = Number of correct predictions / Total number of predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4e256",
   "metadata": {},
   "source": [
    "### Problem with Accuracy Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000878f",
   "metadata": {},
   "source": [
    "* It doesn't tel the nature of mistake ( means that it not tells that erros type is 1 OR 2). That why we use Confusion Matrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc226af9",
   "metadata": {},
   "source": [
    "# Confusion Matric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd0fc5",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. ... The classifier made a total of 165 predictions (e.g., 165 patients were being tested for the presence of that disease)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d68f2d",
   "metadata": {},
   "source": [
    "<img title=\"Confusion Metric\" src=\"confusion_matrix2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2a561",
   "metadata": {},
   "source": [
    "But some times confusion metrix doesn't provide us so much accurate results suppose there are 100000 passengers in tha airplane and there is only 1 person is a terrorist so when we calculate the accuracy so it will be 99.99% almost equal to 100% in this scenario the confusion metrix becomes fail so in this case we use the precision and recall method.\n",
    "\n",
    "* It becomes fail in the case of imbalance Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc37277",
   "metadata": {},
   "source": [
    "<img title='Classification Metrics' src='classification metrix.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443592d",
   "metadata": {},
   "source": [
    "[All of the coding sources related to Accuracy score and Confusion Metrix is here](https://github.com/campusx-official/100-days-of-machine-learning/tree/main/day59-classification-metrics \"Click Here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5c872",
   "metadata": {},
   "source": [
    "# Precision Metric\n",
    "Precision is a metric that quantifies the number of correct positive predictions made. Precision, therefore, calculates the accuracy for the minority class. It is calculated as the ratio of correctly predicted positive examples divided by the total number of positive examples that were predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79619f19",
   "metadata": {},
   "source": [
    "<img title=\"Precision Metric\" src='precision metrix.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4c8a5",
   "metadata": {},
   "source": [
    "# Recall Metric\n",
    "Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made. Unlike precision that only comments on the correct positive predictions out of all positive predictions, recall provides an indication of missed positive predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba2d19",
   "metadata": {},
   "source": [
    "<img title='Recall Metric' src='recall metrix.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1deb68",
   "metadata": {},
   "source": [
    "# Macro Recall, Precision, F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6f0b8",
   "metadata": {},
   "source": [
    "<img title='Macro_recall, Macro_Precision and Macro_F1 Score' src='macro recall & precision.png' height=500 width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ffb74",
   "metadata": {},
   "source": [
    "<img src='macro recall.png' hwight=600 width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d4f58d",
   "metadata": {},
   "source": [
    "# F1 Score\n",
    "F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6f3b5",
   "metadata": {},
   "source": [
    "<img title='F1 Score' src='f1 score.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b45036",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "The Softmax regression is a form of logistic regression that normalizes an input value into a vector of values that follows a probability distribution whose total sums up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d365f16",
   "metadata": {},
   "source": [
    "<img title='Softmax Regression' src='softmax_function.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5a6de",
   "metadata": {},
   "source": [
    "# Loss function for Softmax Regression \n",
    "<img title='Cost Function for Softmax Regression' src='Cost+Function.jpg' height=600 width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb3cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e3f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d85bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8ed1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
